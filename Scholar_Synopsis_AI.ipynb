{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Import necessary libraries"
      ],
      "metadata": {
        "id": "bvvH7r-qdzYE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oIhoF_7--FUn"
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries\n",
        "from ibm_watsonx_ai.foundation_models import ModelInference\n",
        "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
        "from ibm_watsonx_ai.metanames import EmbedTextParamsMetaNames\n",
        "from ibm_watsonx_ai import Credentials\n",
        "from langchain_ibm import WatsonxLLM, WatsonxEmbeddings\n",
        "\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import Language, RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "# You can use this section to suppress warnings generated by your code:\n",
        "def warn(*args, **kwargs):\n",
        "    pass\n",
        "import warnings\n",
        "warnings.warn = warn\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###LLM\n",
        "Initialize the LLM by creating an instance of WatsonxLLM, a class in langchain_ibm. WatsonxLLM can use several underlying foundational models. In this project, Mixtral 8x7B is used.\n",
        "The model is initialized with temperature= 0.5, and a maximum token generation of 256 tokens."
      ],
      "metadata": {
        "id": "xxQyJuocYcWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## LLM\n",
        "def get_llm():\n",
        "    model_id = 'mistralai/mixtral-8x7b-instruct-v01'\n",
        "    parameters = {\n",
        "        GenParams.MAX_NEW_TOKENS: 256,\n",
        "        GenParams.TEMPERATURE: 0.5,\n",
        "    }\n",
        "    project_id = \"Give project id here\"\n",
        "    watsonx_llm = WatsonxLLM(\n",
        "        model_id=model_id,\n",
        "        url=\"Give URL here\",\n",
        "        project_id=project_id,\n",
        "        params=parameters,\n",
        "    )\n",
        "    return watsonx_llm"
      ],
      "metadata": {
        "id": "qwVMDKXKYh2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Document loader  \n",
        "Function to load the pdf file. PyPDFLoader is used in this project."
      ],
      "metadata": {
        "id": "XNcVI2GMd5DR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Document loader function to load pdf file\n",
        "def document_loader(file):\n",
        "  loader= PyPDFLoader(file)\n",
        "  loaded_doc= loader.load()\n",
        "  return loaded_doc"
      ],
      "metadata": {
        "id": "coU2PajM-YAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading a sample research document\n",
        "file='/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf'\n",
        "sample_doc= document_loader(file)"
      ],
      "metadata": {
        "id": "5QALKm3sH319"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Text Splitter\n",
        "Function to split the loaded pdf file into chunks. RecursiveCharacterTextSplitter is used to split the doc into chunks."
      ],
      "metadata": {
        "id": "eMCJ4FnveHRk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Text Splitter to chunk the loaded pdf file\n",
        "def text_splitter(loaded_doc,chunk_size, chunk_overlap):\n",
        "  doc_splitter = RecursiveCharacterTextSplitter.from_language(\n",
        "    language=Language.MARKDOWN, chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
        "    )\n",
        "  chunks = doc_splitter.split_documents(loaded_doc)\n",
        "  return chunks"
      ],
      "metadata": {
        "id": "C6EpbA89OCiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_chunks= text_splitter(sample_doc,250,20)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Ie2fvURzZMtu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(sample_chunks))\n",
        "print(sample_chunks[10].page_content)\n",
        "print(type(sample_chunks[10]))\n",
        "print(type(sample_chunks))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6EKbYanbAUBN",
        "outputId": "1b3071d0-a5e2-4ec7-a4d5-610f09ee3cf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "124\n",
            "model depth or reducing the usable sequence length. Furthermore, these meth-\n",
            "ods typically do not perform as well as full fine-tuning, leading to a trade-off\n",
            "between efficiency and model performance.\n",
            "<class 'langchain_core.documents.base.Document'>\n",
            "<class 'list'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embedding Model\n",
        "Embeddings are generated using IBM's Slate 125M English embeddings model.\n",
        "\n"
      ],
      "metadata": {
        "id": "N0w0aCMVeyHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Embedding model\n",
        "def watsonx_embedding():\n",
        "    embed_params = {\n",
        "        EmbedTextParamsMetaNames.TRUNCATE_INPUT_TOKENS: 3,\n",
        "        EmbedTextParamsMetaNames.RETURN_OPTIONS: {\"input_text\": True},\n",
        "    }\n",
        "    watsonx_embedding = WatsonxEmbeddings(\n",
        "        model_id=\"ibm/slate-125m-english-rtrvr\",\n",
        "        url=\"https://us-south.ml.cloud.ibm.com\",\n",
        "        project_id=\"skills-network\",\n",
        "        params=embed_params,\n",
        "    )\n",
        "    return watsonx_embedding"
      ],
      "metadata": {
        "id": "DlNmJnpJV5wz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vector Store\n",
        "Chroma vector store is used to store the generated vector embeddings"
      ],
      "metadata": {
        "id": "5DTPssQgA9li"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vector_database(chunks):\n",
        "    embedding_model = watsonx_embedding()\n",
        "    vectordb = Chroma.from_documents(chunks, embedding_model)\n",
        "    return vectordb"
      ],
      "metadata": {
        "id": "5xbysKXQAoiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db = vector_database(sample_chunks)"
      ],
      "metadata": {
        "id": "a0RkMnYAAoMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(db))\n",
        "print(type(db))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYDAVaRCFWEi",
        "outputId": "3e669b8f-f614-4391-e44c-f4278bbb8012"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "124\n",
            "<class 'langchain_community.vectorstores.chroma.Chroma'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Retriever\n",
        "A retriever is an interface designed to return documents based on an unstructured query. Unlike a vector store, which stores and retrieves documents, a retriever's primary function is to find and return relevant documents. While vector stores can serve as the backbone of a retriever, there are various other types of retrievers that can be used as well.\n",
        "\n",
        "Retrievers take a string `query` as input and output a list of `Documents`.\n"
      ],
      "metadata": {
        "id": "IuPiWJVNHNPk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Retriever\n",
        "def retriever(file):\n",
        "    splits = document_loader(file)\n",
        "    chunks = text_splitter(splits)\n",
        "    vectordb = vector_database(chunks)\n",
        "    retriever = vectordb.as_retriever(search_type=\"similarity\",search_kwargs={\"k\": 4})\n",
        "    return retriever"
      ],
      "metadata": {
        "id": "voxuvU_rFWBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Question-Answering Chain\n",
        "In this project, `RetrievalQA` from langchain, a chain that performs natural-language question-answering over a data source using retrieval-augmented generation (RAG), is used."
      ],
      "metadata": {
        "id": "oW-od915XM0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RetrievalQA Chain\n",
        "# LLM and retriever obj are taken by RetrievalQA\n",
        "def retriever_qa(doc, query):\n",
        "  llm= get_llm()\n",
        "  retriever_obj= retriever(doc)\n",
        "  qa= RetrievalQA.from_chain_type(llm=llm,\n",
        "                                  chain_type=\"stuff\",\n",
        "                                  retriever= retriever_obj,\n",
        "                                  return_source_documents=False)\n",
        "  result= qa.invoke(query)\n",
        "  return response['result']"
      ],
      "metadata": {
        "id": "gdVTTeNYFV-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set up the Gradio interface\n",
        "The web interface is user customizable and will have the following:\n",
        "*  A file upload functionality (provided by the `File` class in Gradio)\n",
        "*  An input textbox where the question can be asked (provided by the `Textbox` class in Gradio)\n",
        "* An output textbox where the question can be answered (provided by the `Textbox` class in Gradio)\n",
        "\n"
      ],
      "metadata": {
        "id": "hlgjN2GSj_ds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the Gradio interface\n",
        "rag_application= gr.Interface(\n",
        "    fn= retriever_qa,\n",
        "    allow_flagging=\"never\",\n",
        "    inputs=[\n",
        "    gr.File(label=\"Upload PDF File\", file_count=\"single\", file_types=['.pdf'], type=\"filepath\"),  # Drag and drop file upload\n",
        "        gr.Textbox(label=\"Input Query\", lines=2, placeholder=\"Type your question here...\")  # Input query box\n",
        "    ],\n",
        "    outputs= gr.Textbox(label=\"Summary of the Research Paper\"), # Output response box\n",
        "    title= \"Scholar Synopsis AI\",\n",
        "    description=\"Upload a research paper. The assistant can summarize and answer any questions you have to help you grasp key ideas and insights regarding the paper.\"\n",
        ")"
      ],
      "metadata": {
        "id": "3H377g4jFV6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qCmvh00tFVkG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}