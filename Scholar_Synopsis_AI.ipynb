{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Import necessary libraries"
      ],
      "metadata": {
        "id": "bvvH7r-qdzYE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oIhoF_7--FUn"
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries\n",
        "from ibm_watsonx_ai.foundation_models import ModelInference\n",
        "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
        "from ibm_watsonx_ai.metanames import EmbedTextParamsMetaNames\n",
        "from ibm_watsonx_ai import Credentials\n",
        "from langchain_ibm import WatsonxLLM, WatsonxEmbeddings\n",
        "\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import Language, RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "# You can use this section to suppress warnings generated by your code:\n",
        "def warn(*args, **kwargs):\n",
        "    pass\n",
        "import warnings\n",
        "warnings.warn = warn\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###LLM\n",
        "Initialize the LLM by creating an instance of WatsonxLLM, a class in langchain_ibm. WatsonxLLM can use several underlying foundational models. In this project, Mixtral 8x7B is used.\n",
        "The model is initialized with temperature= 0.5, and a maximum token generation of 256 tokens."
      ],
      "metadata": {
        "id": "xxQyJuocYcWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## LLM\n",
        "def get_llm():\n",
        "    model_id = 'mistralai/mixtral-8x7b-instruct-v01'\n",
        "    parameters = {\n",
        "        GenParams.MAX_NEW_TOKENS: 256,\n",
        "        GenParams.TEMPERATURE: 0.5,\n",
        "    }\n",
        "    project_id = \"skills-network\"\n",
        "    watsonx_llm = WatsonxLLM(\n",
        "        model_id=model_id,\n",
        "        url=\"https://us-south.ml.cloud.ibm.com\",\n",
        "        project_id=project_id,\n",
        "        params=parameters,\n",
        "    )\n",
        "    return watsonx_llm"
      ],
      "metadata": {
        "id": "qwVMDKXKYh2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Document loader  \n",
        "Function to load the pdf file. PyPDFLoader is used in this project."
      ],
      "metadata": {
        "id": "XNcVI2GMd5DR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Document loader function to load pdf file\n",
        "def document_loader(file):\n",
        "  loader= PyPDFLoader(file)\n",
        "  loaded_doc= loader.load()\n",
        "  return loaded_doc"
      ],
      "metadata": {
        "id": "coU2PajM-YAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading a sample research document\n",
        "file='/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf'\n",
        "sample_doc= document_loader(file)"
      ],
      "metadata": {
        "id": "5QALKm3sH319"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_doc # each page is a Document object"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4g3iKX7JQ_7s",
        "outputId": "4d43e264-9941-489b-d715-428adb979ed5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 0}, page_content='A Comprehensive Review of Low-Rank\\nAdaptation in Large Language Models for\\nEfficient Parameter Tuning\\nSeptember 10, 2024\\nAbstract\\nNatural Language Processing (NLP) often involves pre-training large\\nmodels on extensive datasets and then adapting them for specific tasks\\nthrough fine-tuning. However, as these models grow larger, like GPT-3\\nwith 175 billion parameters, fully fine-tuning them becomes computa-\\ntionally expensive. We propose a novel method called LoRA (Low-Rank\\nAdaptation) that significantly reduces the overhead by freezing the orig-\\ninal model weights and only training small rank decomposition matrices.\\nThis leads to up to 10,000 times fewer trainable parameters and reduces\\nGPU memory usage by three times. LoRA not only maintains but some-\\ntimes surpasses fine-tuning performance on models like RoBERTa, De-\\nBERTa, GPT-2, and GPT-3. Unlike other methods, LoRA introduces\\nno extra latency during inference, making it more efficient for practical\\napplications. All relevant code and model checkpoints are available at\\nhttps://github.com/microsoft/LoRA.\\n1 Introduction\\nMany natural language processing (NLP) applications rely on adapting large,\\npre-trained language models for various downstream tasks. Typically, this is\\ndone through fine-tuning, where all the parameters of the pre-trained model are\\nupdated. However, a significant drawback of fine-tuning is that the adapted\\nmodel has just as many parameters as the original one. As models grow in size,\\nwhat was once a manageable issue for models like GPT-2 or RoBERTa large\\nbecomes a serious deployment challenge with larger models like GPT-3, which\\nhas 175 billion trainable parameters.\\nTo mitigate these challenges, researchers have explored adapting only cer-\\ntain parts of the model or adding external modules specific to each task. This\\napproach reduces the need to store and manage large numbers of parameters\\nfor each task, greatly improving efficiency during deployment. However, cur-\\nrent methods often introduce drawbacks, such as inference delays by increasing\\n1'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 1}, page_content='model depth or reducing the usable sequence length. Furthermore, these meth-\\nods typically do not perform as well as full fine-tuning, leading to a trade-off\\nbetween efficiency and model performance.\\nInspired by prior works that demonstrate over-parametrized models often\\nreside in a low intrinsic dimensional space, we hypothesize that weight changes\\nduring model adaptation also have a low “intrinsic rank.” This insight leads to\\nour Low-Rank Adaptation (LoRA) approach. LoRA optimizes low-rank decom-\\nposition matrices for the dense layers’ weight changes during adaptation, while\\nkeeping the pre-trained weights frozen. As illustrated in Figure 1, even with\\nlarge models like GPT-3 (with up to 12,288 dimensions in full rank), a low-rank\\nmatrix (rank 1 or 2) is sufficient, making LoRA highly efficient in terms of both\\nstorage and computation.\\nLoRA has several notable advantages:\\n•The pre-trained model can be shared, and small LoRA modules can be\\ncreated for various tasks. By freezing the main model and only switching\\nthe matrices AandB(shown in Figure 1), storage and task-switching\\noverhead are significantly reduced.\\n•LoRA improves training efficiency and reduces hardware requirements,\\nlowering the entry barrier by up to threefold when using adaptive opti-\\nmizers. This is because LoRA only requires updating the smaller low-rank\\nmatrices, avoiding the need to calculate gradients for most parameters.\\n•The simple linear design allows merging of the trainable matrices with\\nthe frozen pre-trained weights during deployment, ensuring no additional\\ninference latency compared to fully fine-tuned models.\\n•LoRA is compatible with many existing methods and can be combined\\nwith approaches like prefix-tuning.\\nIn this work, we follow standard conventions for Transformer architecture\\nand refer to dimensions such as dmodel, and projection matrices like Wq,Wk,Wv,\\nandWofor the self-attention module. WorW0represents a pre-trained weight\\nmatrix, while ∆ Wrefers to its update during adaptation. The rank rdenotes\\nthe rank of a LoRA module. Throughout, we use Adam for optimization and\\nmaintain the Transformer MLP feedforward dimension as dffn= 4×dmodel.\\n1.1 Key Advantages of LoRA\\n•Efficient Task Switching : A pre-trained model can support multiple\\ntasks by swapping the small LoRA matrices, reducing storage needs.\\n•Reduced Hardware Requirements : LoRA lowers the GPU memory\\nneeded for training by freezing most parameters and only training the\\nlow-rank matrices.\\n•No Additional Latency : LoRA incurs no extra inference delay because\\nthe matrices can be merged with the pre-trained weights when deployed.\\n2'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 2}, page_content='•Combining with Other Methods : LoRA can be used with other ap-\\nproaches, like prefix-tuning, to further optimize model performance.\\n2 Problem Statement\\nAlthough our approach is independent of the specific training objective, we focus\\non language modeling as the central application. Below, we outline the key\\naspects of the language modeling problem, particularly the goal of maximizing\\nconditional probabilities based on task-specific prompts.\\nAssume we have an autoregressive language model PΦ(y|x) that is pre-\\ntrained and parameterized by Φ. For example, PΦ(y|x) could be a general\\nmulti-task model such as GPT, built on top of the Transformer architecture.\\nThe model can then be adapted to different downstream tasks such as text\\nsummarization, machine reading comprehension (MRC), and natural language\\nto SQL (NL2SQL). Each downstream task is represented as a training set of\\ncontext-output pairs:\\nZ={(xi, yi)}i=1,...,N,\\nwhere both xiandyiare sequences of tokens. For instance, in NL2SQL, xi\\nmight represent a natural language question and yiwould be the corresponding\\nSQL query; in summarization, xirepresents the article and yiwould be its\\nsummary.\\nIn traditional fine-tuning, the model is initialized using the pre-trained weights\\nΦ0, which are then updated to Φ 0+ ∆Φ by optimizing the model’s parameters\\nto maximize the conditional probabilities for each token:\\nmax\\nΦX\\n(x,y)∈Z|y|X\\nt=1log (PΦ(yt|x, y<t)) (1)\\nA significant limitation of full fine-tuning is that for every downstream task, a\\ndifferent set of parameters ∆Φ must be learned, and the size of ∆Φ is equal to the\\nsize of Φ 0. For large models, such as GPT-3 with 175 billion parameters, storing\\nand deploying multiple instances of fine-tuned models becomes impractical or\\nextremely challenging.\\nTo address this issue, we propose a more efficient approach where the task-\\nspecific parameter updates ∆Φ = ∆Φ(Θ) are encoded using a much smaller set\\nof parameters Θ, where |Θ| ≪ |Φ0|. As a result, optimizing the model for each\\ntask reduces to optimizing Θ as follows:\\nmax\\nΘX\\n(x,y)∈Z|y|X\\nt=1log\\x00\\npΦ0+∆Φ(Θ) (yt|x, y<t)\\x01\\n(2)\\nIn the following sections, we explore a low-rank approach for representing\\n∆Φ, making the adaptation process more efficient in both computational and\\nmemory terms. For large models like GPT-3 175B, this method allows the\\ntrainable parameters |Θ|to be reduced to as little as 0.01% of |Φ0|.\\n3'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 3}, page_content='3 Limitations on Current Solutions\\nThe challenge we aim to address is not new. Since the rise of transfer learning,\\na great deal of work has focused on making model adaptation more efficient in\\nterms of both parameters and computation. For an overview, see Section 6 for\\nsome well-known works. Focusing on language modeling, two prominent strate-\\ngies for efficient adaptation stand out: adding adapter layers, or optimizing the\\ninput layer activations. However, both approaches come with limitations, espe-\\ncially when applied in large-scale, latency-sensitive production environments.\\n3.1 Adapter Layers and Inference Latency\\nThere are many variations of adapters. We focus on the original adapter design\\nfrom [ ?], which introduces two adapter layers per Transformer block, and a\\nmore recent approach by [ ?], which only uses one adapter per block but with an\\nadditional LayerNorm [ ?]. Although overall latency can be reduced by pruning\\nlayers or leveraging multi-task settings [ ?], [?], there is no way to completely\\neliminate the additional computation introduced by adapter layers. This might\\nseem minor since adapters generally have few parameters (typically less than\\n1% of the original model) due to their small bottleneck dimension, which limits\\nthe number of floating-point operations (FLOPs). However, large-scale neural\\nnetworks rely heavily on parallel processing to maintain low latency, and adapter\\nlayers are processed sequentially. This becomes more evident in scenarios with\\nlow batch sizes, such as real-time inference, where models like GPT-2 [ ?] running\\non a single GPU experience noticeable increases in latency, even with small\\nbottleneck dimensions (Table 1).\\nThe issue is further compounded when models need to be sharded across\\nmultiple devices, since the increased model depth requires more synchronous\\nGPU operations like AllReduce andBroadcast , unless adapter parameters are\\nredundantly replicated.\\n3.2 Challenges with Directly Optimizing the Prompt\\nAnother approach, such as prefix tuning [ ?], faces a different challenge. We\\nhave observed that prefix tuning is often difficult to optimize and that its per-\\nformance does not consistently improve as more trainable parameters are added,\\nconfirming earlier findings. Moreover, allocating part of the sequence length for\\nadaptation inevitably reduces the available sequence length for processing task-\\nrelated data, which seems to hinder prompt tuning’s performance compared to\\nother methods. We will further explore this issue in Section 5.\\n4'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 4}, page_content='Batch Size Sequence Length |Θ| Latency (ms)\\nFine-Tune/LoRA\\n32 512 0.5M 1449.4 ±0.8\\n16 256 11M 338.0±0.6\\n1 128 11M 19.8±2.7\\nAdapterL\\n32 512 0.5M 1482.0 ±1.0 (+2.2%)\\n16 256 11M 354.8±0.5 (+5.0%)\\n1 128 11M 23.9±2.1 (+20.7%)\\nAdapterH\\n32 512 0.5M 1492.2 ±1.0 (+3.0%)\\n16 256 11M 366.3±0.5 (+8.4%)\\n1 128 11M 25.8±2.2 (+30.3%)\\nTable 1: Inference latency of a forward pass in GPT-2 Medium measured over\\n100 trials using an NVIDIA Quadro RTX8000. ” |Θ|” refers to the number of\\ntrainable parameters in the adapter layers. AdapterLand AdapterHare two\\ntypes of adapter tuning. The impact on latency becomes significant, particularly\\nin online scenarios with shorter sequences and smaller batch sizes.\\n4 Our Method\\nIn this section, we explain the structure of LoRA and its practical benefits.\\nThe principles outlined here apply generally to dense layers in neural networks,\\nalthough we focus on specific weights in Transformer language models, as these\\nmodels serve as the central example in our experiments.\\n4.1 Low-Rank Parameterized Update Matrices\\nNeural networks contain numerous dense layers that perform matrix multipli-\\ncation, and the weight matrices in these layers typically have a full rank. When\\nadapting to a particular task, it shows that pre-trained language models pos-\\nsess a low ”intrinsic dimension” and can still perform effectively after a random\\nprojection to a smaller subspace. Drawing inspiration from this, we hypothesize\\nthat updates to the weights during adaptation also have a low ”intrinsic rank.”\\nFor a pre-trained weight matrix W0∈Rd×k, we limit its update by express-\\ning it as a low-rank decomposition, W0+ ∆W=W0+BA, where B∈Rd×r,\\nA∈Rr×k, and the rank r≪min(d, k). During training, W0is fixed, and A\\nandBare the trainable parameters. Both W0and ∆ W=BAare multiplied\\nwith the input, and their respective outputs are summed element-wise. Thus,\\nforh=W0x, our updated forward pass becomes:\\nh=W0x+ ∆Wx=W0x+BAx\\nWe illustrate this reparametrization in Figure 1. We initialize Awith random\\nGaussian values and set Bto zero, meaning ∆ W=BAis zero at the start\\n5'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 5}, page_content='of training. We then scale ∆ Wxbyα\\nr, where αis a constant dependent on\\nr. When using Adam for optimization, adjusting αhas an effect similar to\\ntuning the learning rate. Therefore, we use the same αfor our first experiments\\nand avoid tuning it. This scaling method also minimizes the need to adjust\\nhyperparameters when varying r.\\n4.1.1 A Generalization of Full Fine-Tuning\\nA more general fine-tuning technique involves training only a subset of pre-\\ntrained parameters. LoRA extends this approach by eliminating the need for\\nfull-rank gradient updates to weight matrices. Instead, LoRA uses low-rank\\nmatrices for adaptation. If LoRA is applied to all weight matrices, and all\\nbiases are trained, the expressiveness of full fine-tuning is recovered by setting\\nthe LoRA rank requal to the rank of the pre-trained weight matrices. As the\\nnumber of trainable parameters increases, LoRA approaches the full fine-tuning\\nperformance, while adapter-based techniques converge to simpler models that\\ncannot process long input sequences.\\n4.1.2 No Additional Inference Latency\\nWhen deploying LoRA, we can explicitly compute W=W0+BAand use it\\nduring inference. This means that when switching between tasks, we can quickly\\nsubtract BAand add a different low-rank matrix B′A′without consuming ex-\\ntra memory. This ensures that no additional inference latency is introduced\\ncompared to fully fine-tuned models.\\n4.2 Applying LoRA to Transformer Models\\nIn principle, LoRA can be applied to any subset of weight matrices in a neural\\nnetwork to minimize the number of trainable parameters. In a Transformer\\narchitecture, the self-attention module contains four projection matrices Wq,\\nWk,Wv, and Wo, and the MLP module contains two more matrices. We treat\\nthe weight matrices in the self-attention module as a single dmodel×dmodel\\nmatrix, despite them being split into different attention heads. To simplify\\nthe process and improve parameter efficiency, we restrict our method to only\\nadapting the attention weights for downstream tasks, leaving the MLP module\\nfrozen. The effect of adapting various attention weight matrices in a Transformer\\nis further explored in Section 7.1. We leave the investigation of adapting MLP\\nlayers, LayerNorm layers, and biases to future work.\\n4.2.1 Practical Benefits and Limitations\\nOne of the major advantages of LoRA is its reduction in memory and storage\\ncosts. For large Transformer models using Adam, LoRA can cut VRAM usage\\nby up to two-thirds if r≪dmodel, since it eliminates the need to store optimizer\\nstates for frozen parameters. For example, with GPT-3 175B, VRAM usage\\nduring training drops from 1.2 TB to 350 GB. With r= 4, and only the query\\n6'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 6}, page_content='and value matrices being adapted, the checkpoint size decreases by approxi-\\nmately 10,000 ×(from 350 GB to 35 MB). This makes it possible to train using\\nsignificantly fewer GPUs and avoid I/O bottlenecks. LoRA also enables easier\\ntask-switching during deployment by simply swapping out the LoRA weights,\\nwhich requires far less memory than loading entirely new model parameters.\\nAdditionally, LoRA offers a 25% training speedup compared to full fine-tuning\\nbecause there is no need to compute gradients for most parameters.\\nHowever, LoRA does have some limitations. It is not straightforward to\\ncombine multiple tasks with different low-rank matrices AandBin a single\\nforward pass if BAis absorbed into Wto remove additional inference latency.\\nWhile it is possible to dynamically select LoRA modules during inference, this\\nsolution is not suitable for scenarios where low-latency responses are crucial.\\n5 Empirical Experiments\\nWe assess LoRA’s performance in downstream tasks across several models in-\\ncluding RoBERTa, DeBERTa, and GPT-2, before scaling up to GPT-3 175B.\\nOur experiments cover various tasks, ranging from natural language understand-\\ning (NLU) to natural language generation (NLG). For RoBERTa and DeBERTa,\\nwe evaluate on the GLUE benchmark. All experiments were performed using\\nNVIDIA Tesla V100 GPUs.\\n5.1 Baselines\\nFor comparison with a wide range of baselines, we replicate experimental setups\\nfrom previous studies and, where possible, reuse reported results. This might\\nresult in some baselines being present in only a subset of experiments.\\nFine-Tuning (FT) is a common method for adapting models. During fine-\\ntuning, the model’s pre-trained weights and biases are updated using gradient\\ndescent. A variant of this is fine-tuning only select layers, while freezing the\\nrest. One such baseline from prior work on GPT-2 updates only the last two\\nlayers (denoted as FTTop2).\\nBitFit is another baseline in which only the bias parameters are updated,\\nwhile all other parameters remain frozen. This method has gained attention,\\nincluding in recent studies [ ?].\\nPrefix-embedding tuning (PreEmbed) involves adding special tokens to\\nthe input sequence, and training their embeddings. These tokens do not belong\\nto the model’s original vocabulary. Their placement—either prepended (prefix)\\nor appended (infix)—can significantly affect performance, as highlighted in [ ?].\\nPrefix-layer tuning (PreLayer) extends prefix tuning by learning train-\\nable activations at each Transformer layer. This results in a larger number of\\ntrainable parameters, as activations from prior layers are progressively replaced.\\nThe total number of trainable parameters is given by |Θ|=L×dmodel×(lp+li),\\nwhere Lis the number of Transformer layers.\\n7'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 7}, page_content='Adapter tuning [?] introduces additional fully connected adapter layers\\nbetween existing layers in the Transformer. Several variants exist, such as\\nAdapterH and AdapterL [ ?], which differ in the placement of adapters within\\nthe network. The number of trainable parameters in these methods is |Θ|=\\nLAdpt×(2×dmodel×r+r+dmodel) + 2×LLN×dmodel.\\nLoRA , on the other hand, introduces trainable low-rank matrices to the ex-\\nisting weight matrices. As detailed in Section 4.2, LoRA is applied to the query\\nand value matrices in most experiments. The number of trainable parameters\\nis determined by the rank rand the shape of the original weight matrices:\\n|Θ|= 2×LLoRA×dmodel×r, where LLoRA represents the number of weight\\nmatrices to which LoRA is applied.\\nTable 2: GPT-2 Medium and Large results on E2E NLG Challenge. Higher\\nscores are better for all metrics. Confidence intervals are provided for experi-\\nments we conducted. *Results from prior work.\\nModel & Method # Trainable Parameters BLEU NIST MET ROUGE-L CIDEr\\nGPT-2 M (FT)* 354.92M 68.2 8.62 46.2 71.0 2.47\\nGPT-2 M (LoRA) 0.35M 70.4±0.1 8.85±0.2 46.8±0.2 71.8±0.1 2.53±0.2\\nGPT-2 L (FT)* 774.03M 68.5 8.78 46.0 69.9 2.45\\nGPT-2 L (LoRA) 0.77M 70.4±0.1 8.89±0.2 46.8±0.2 72.0±0.2 2.47±0.2\\n5.2 Scaling LoRA to GPT-3 175B\\nTo further test LoRA’s scalability, we apply it to GPT-3 175B. Given the large\\ncomputational cost of GPT-3, we only report standard deviations for each task\\nbased on multiple random seeds. See Appendix D.4 for hyperparameters used.\\nAs presented in Table ??, LoRA matches or outperforms full fine-tuning on\\nWikiSQL, MultiNLI, and SAMSum. Notably, we observe that certain methods\\ndo not consistently benefit from increasing the number of trainable parame-\\nters. As shown in Figure 1, LoRA remains efficient even at low ranks, avoiding\\nthe performance degradation seen with larger token embeddings in prefix-based\\nmethods.\\nFigure 1: GPT-3 175B validation accuracy vs. the number of trainable parame-\\nters for several adaptation methods on WikiSQL and MNLI. LoRA demonstrates\\nbetter scalability and performance.\\n8'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 8}, page_content='6 Related Works\\n6.1 Transformer Language Models\\nThe Transformer architecture, as introduced by Vaswani et al. (2017), has\\nproven to be a highly effective sequence-to-sequence model due to its heavy use\\nof self-attention mechanisms. Radford et al. (2018) applied it to autoregressive\\nlanguage modeling, significantly boosting its utility in the field. Since then,\\nTransformer-based models have become a staple in natural language processing\\n(NLP), achieving state-of-the-art results in a wide variety of tasks. Notably,\\nBERT (Devlin et al., 2018) and GPT-2 (Radford et al., 2019) have paved the\\nway for large-scale pre-trained language models that, when fine-tuned, deliver\\nexcellent performance on specific tasks. The next breakthrough came with GPT-\\n3 (Brown et al., 2020), which is currently the largest single Transformer language\\nmodel with 175 billion parameters.\\n6.2 Prompt Engineering and Fine-Tuning\\nDespite GPT-3’s ability to adapt its behavior with minimal data (few-shot learn-\\ning), its performance is highly sensitive to how the input prompt is structured\\n(Brown et al., 2020). This has led to the rise of ”prompt engineering,” a process\\nthat involves crafting and fine-tuning the input prompts to maximize model per-\\nformance on specific tasks. Fine-tuning, on the other hand, refers to retraining\\na model pre-trained on general domains to adapt it to a particular task (Devlin\\net al., 2018; Radford et al., 2018). Some approaches only update a subset of the\\nmodel’s parameters (Collobert and Weston, 2008), but it is common practice to\\nfine-tune all parameters to achieve the best performance. However, performing\\nfull fine-tuning on a model as large as GPT-3, with its 175 billion parameters,\\nposes significant challenges due to the large memory requirements and the com-\\nputational resources needed, making it as resource-intensive as pre-training.\\n6.3 Parameter-Efficient Adaptation\\nMany techniques have been developed to address the inefficiency of full fine-\\ntuning by adapting only certain layers or introducing adapter modules. Houlsby\\net al. (2019), Rebuffi et al. (2017), and Lin et al. (2020) proposed inserting\\nadapter layers between existing layers in the network. These adapters allow for\\nparameter-efficient adaptation by learning only a small number of task-specific\\nparameters. Our method imposes a low-rank constraint on the weight updates,\\nensuring that learned weights can be merged with the main model weights dur-\\ning inference, thus introducing no additional latency, unlike the adapter layers.\\nA related approach, COMPACTER (Mahabadi et al., 2021), uses Kronecker\\nproducts to parametrize the adapters, further improving parameter efficiency.\\nAdditionally, prompt optimization techniques, such as those proposed by Li and\\nLiang (2021), Lester et al. (2021), and Hambardzumyan et al. (2020), aim to\\noptimize the input tokens directly. However, these approaches typically reduce\\n9'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 9}, page_content='the available sequence length for task processing. Our work can be combined\\nwith such methods for further gains in efficiency.\\n6.4 Low-Rank Structures in Deep Learning\\nLow-rank structures are prevalent in many machine learning problems, and sev-\\neral studies have explored imposing these constraints on deep models. Li et\\nal. (2016), Cai et al. (2010), and Grasedyck et al. (2013) showed that many\\nlearning tasks have an intrinsic low-rank structure. For deep neural networks,\\nparticularly over-parameterized models, it has been shown that they often ex-\\nhibit low-rank properties after training (Oymak et al., 2019). Prior works, such\\nas those by Sainath et al. (2013), Zhang et al. (2014), and Denil et al. (2014),\\nhave explicitly imposed low-rank constraints during training to enhance model\\nefficiency. However, our approach differs in that we apply low-rank updates to\\nfrozen pre-trained models, making it highly effective for task-specific adapta-\\ntion. Neural networks with low-rank structures have been shown to outperform\\nclassical methods such as finite-width neural tangent kernels (Allen-Zhu et al.,\\n2019; Li and Liang, 2018), and low-rank adaptations are particularly useful in\\nadversarial training scenarios (Allen-Zhu and Li, 2020). This makes our pro-\\nposed low-rank adaptation well-grounded in both theory and practice.\\n7 Analyzing Low-Rank Adaptations\\nIn light of the demonstrated benefits of LoRA, we aim to further explore the\\nattributes of low-rank adaptation as applied to various downstream tasks. The\\nlow-rank structure does not only reduce the hardware requirements for conduct-\\ning parallel experiments, but it also provides better insight into how adapted\\nweights align with pre-trained weights. Our focus lies on GPT-3 175B, where\\nwe managed to significantly reduce the number of trainable parameters (up to\\n10,000 ×) without sacrificing task performance.\\nIn this section, we address some key questions:\\n•1)With a constrained parameter budget, which weight matrices should\\nbe adapted to achieve the best downstream task performance?\\n•2)Is the adapted matrix ∆ Wtruly rank-deficient, and if so, what rank is\\noptimal for practical use?\\n•3)How is ∆ Wrelated to the pre-trained weights W? Does ∆ Wexhibit\\nhigh correlation with W, and what is the comparative size of ∆ WtoW?\\nThe answers to these questions provide valuable insights for optimizing pre-\\ntrained models for downstream tasks.\\n7.1 Selecting Optimal Weight Matrices for LoRA\\nTo optimize performance under a limited parameter budget, we explore adapting\\ndifferent weight matrices within the self-attention module of the Transformer.\\n10'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 10}, page_content='We allocate 18M parameters (approximately 35MB stored in FP16) for GPT-3\\n175B, using a rank r= 8 for one attention weight type or r= 4 for two types.\\nThe results are displayed in Table 3.\\nTable 3: Validation accuracy on WikiSQL and MultiNLI with LoRA applied to\\ndifferent attention weights in GPT-3, with a fixed number of trainable parame-\\nters.\\n# of Trainable Parameters = 18M WqWkWvWoWq, Wv\\nRank r= 8 70.4 70.0 73.0 73.2 73.7\\nMultiNLI ( ±0.1%) 91.0 90.8 91.0 91.3 91.7\\n7.2 Determining the Ideal Rank for LoRA\\nTo analyze the impact of the rank ron task performance, we applied LoRA with\\nvarying ranks across different combinations of attention matrices. The results\\ncan be found in Table 4.\\nTable 4: Validation accuracy on WikiSQL and MultiNLI with different ranks r.\\nWeight Type r= 1 r= 2 r= 4 r= 8 r= 64\\nWikiSQL ( ±0.5%)Wq68.8 69.6 70.5 70.4 70.0\\nWikiSQL ( Wq, Wv) 73.4 73.3 73.7 73.8 73.5\\nMultiNLI ( ±0.1%)Wq90.7 90.9 91.1 90.7 90.7\\nMultiNLI ( Wq, Wv) 91.3 91.4 91.3 91.6 91.4\\nThe results show that even at a small rank r= 1, LoRA performs well when\\nboth WqandWvare adapted. In contrast, adapting only Wqrequires a higher\\nrank for optimal performance.\\n8 Conclusion\\nLoRA offers a highly efficient solution to the problem of adapting large language\\nmodels for downstream tasks. By freezing the majority of the model’s param-\\neters and training only small, low-rank matrices, LoRA achieves comparable\\nperformance to full fine-tuning while drastically reducing computational costs.\\nIts ability to scale to massive models like GPT-3 without sacrificing performance\\nhighlights its potential for widespread use.\\nFuture work could explore combining LoRA with other parameter-efficient\\nmethods or investigating more principled ways to select which weight matrices\\nto adapt. Additionally, further studies on the rank deficiency of pre-trained\\nweights could inspire new developments in efficient model adaptation.\\n11')]"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing first 1000 characters of the doc\n",
        "print(sample_doc[0].page_content[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "E2mUY9r0IKAN",
        "outputId": "62efd87c-ba64-4f07-ce94-538913d4775a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A Comprehensive Review of Low-Rank\n",
            "Adaptation in Large Language Models for\n",
            "Efficient Parameter Tuning\n",
            "September 10, 2024\n",
            "Abstract\n",
            "Natural Language Processing (NLP) often involves pre-training large\n",
            "models on extensive datasets and then adapting them for specific tasks\n",
            "through fine-tuning. However, as these models grow larger, like GPT-3\n",
            "with 175 billion parameters, fully fine-tuning them becomes computa-\n",
            "tionally expensive. We propose a novel method called LoRA (Low-Rank\n",
            "Adaptation) that significantly reduces the overhead by freezing the orig-\n",
            "inal model weights and only training small rank decomposition matrices.\n",
            "This leads to up to 10,000 times fewer trainable parameters and reduces\n",
            "GPU memory usage by three times. LoRA not only maintains but some-\n",
            "times surpasses fine-tuning performance on models like RoBERTa, De-\n",
            "BERTa, GPT-2, and GPT-3. Unlike other methods, LoRA introduces\n",
            "no extra latency during inference, making it more efficient for practical\n",
            "applications. All relevant code an\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Doc is stored as a list. Each element in the list corresponds to a page in the doc\n",
        "print(type(sample_doc))\n",
        "print(sample_doc[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4Y380Xa0IXtH",
        "outputId": "bbff3360-14dc-4841-b475-f9e998d7a3ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n",
            "page_content='A Comprehensive Review of Low-Rank\n",
            "Adaptation in Large Language Models for\n",
            "Efficient Parameter Tuning\n",
            "September 10, 2024\n",
            "Abstract\n",
            "Natural Language Processing (NLP) often involves pre-training large\n",
            "models on extensive datasets and then adapting them for specific tasks\n",
            "through fine-tuning. However, as these models grow larger, like GPT-3\n",
            "with 175 billion parameters, fully fine-tuning them becomes computa-\n",
            "tionally expensive. We propose a novel method called LoRA (Low-Rank\n",
            "Adaptation) that significantly reduces the overhead by freezing the orig-\n",
            "inal model weights and only training small rank decomposition matrices.\n",
            "This leads to up to 10,000 times fewer trainable parameters and reduces\n",
            "GPU memory usage by three times. LoRA not only maintains but some-\n",
            "times surpasses fine-tuning performance on models like RoBERTa, De-\n",
            "BERTa, GPT-2, and GPT-3. Unlike other methods, LoRA introduces\n",
            "no extra latency during inference, making it more efficient for practical\n",
            "applications. All relevant code and model checkpoints are available at\n",
            "https://github.com/microsoft/LoRA.\n",
            "1 Introduction\n",
            "Many natural language processing (NLP) applications rely on adapting large,\n",
            "pre-trained language models for various downstream tasks. Typically, this is\n",
            "done through fine-tuning, where all the parameters of the pre-trained model are\n",
            "updated. However, a significant drawback of fine-tuning is that the adapted\n",
            "model has just as many parameters as the original one. As models grow in size,\n",
            "what was once a manageable issue for models like GPT-2 or RoBERTa large\n",
            "becomes a serious deployment challenge with larger models like GPT-3, which\n",
            "has 175 billion trainable parameters.\n",
            "To mitigate these challenges, researchers have explored adapting only cer-\n",
            "tain parts of the model or adding external modules specific to each task. This\n",
            "approach reduces the need to store and manage large numbers of parameters\n",
            "for each task, greatly improving efficiency during deployment. However, cur-\n",
            "rent methods often introduce drawbacks, such as inference delays by increasing\n",
            "1' metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Text Splitter\n",
        "Function to split the loaded pdf file into chunks. RecursiveCharacterTextSplitter is used to split the doc into chunks."
      ],
      "metadata": {
        "id": "eMCJ4FnveHRk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Text Splitter to chunk the loaded pdf file\n",
        "def text_splitter(loaded_doc,chunk_size, chunk_overlap):\n",
        "  doc_splitter = RecursiveCharacterTextSplitter.from_language(\n",
        "    language=Language.MARKDOWN, chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
        "    )\n",
        "  chunks = doc_splitter.split_documents(loaded_doc)\n",
        "  return chunks"
      ],
      "metadata": {
        "id": "C6EpbA89OCiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_chunks= text_splitter(sample_doc,250,20)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Ie2fvURzZMtu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_chunks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "hkFPcNF5APd4",
        "outputId": "6c3976e5-d927-4235-c5bb-d4f08137193c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 0}, page_content='A Comprehensive Review of Low-Rank\\nAdaptation in Large Language Models for\\nEfficient Parameter Tuning\\nSeptember 10, 2024\\nAbstract\\nNatural Language Processing (NLP) often involves pre-training large'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 0}, page_content='models on extensive datasets and then adapting them for specific tasks\\nthrough fine-tuning. However, as these models grow larger, like GPT-3\\nwith 175 billion parameters, fully fine-tuning them becomes computa-'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 0}, page_content='tionally expensive. We propose a novel method called LoRA (Low-Rank\\nAdaptation) that significantly reduces the overhead by freezing the orig-\\ninal model weights and only training small rank decomposition matrices.'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 0}, page_content='This leads to up to 10,000 times fewer trainable parameters and reduces\\nGPU memory usage by three times. LoRA not only maintains but some-\\ntimes surpasses fine-tuning performance on models like RoBERTa, De-'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 0}, page_content='BERTa, GPT-2, and GPT-3. Unlike other methods, LoRA introduces\\nno extra latency during inference, making it more efficient for practical\\napplications. All relevant code and model checkpoints are available at\\nhttps://github.com/microsoft/LoRA.'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 0}, page_content='1 Introduction\\nMany natural language processing (NLP) applications rely on adapting large,\\npre-trained language models for various downstream tasks. Typically, this is\\ndone through fine-tuning, where all the parameters of the pre-trained model are'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 0}, page_content='updated. However, a significant drawback of fine-tuning is that the adapted\\nmodel has just as many parameters as the original one. As models grow in size,\\nwhat was once a manageable issue for models like GPT-2 or RoBERTa large'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 0}, page_content='becomes a serious deployment challenge with larger models like GPT-3, which\\nhas 175 billion trainable parameters.\\nTo mitigate these challenges, researchers have explored adapting only cer-'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 0}, page_content='tain parts of the model or adding external modules specific to each task. This\\napproach reduces the need to store and manage large numbers of parameters\\nfor each task, greatly improving efficiency during deployment. However, cur-'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 0}, page_content='rent methods often introduce drawbacks, such as inference delays by increasing\\n1'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 1}, page_content='model depth or reducing the usable sequence length. Furthermore, these meth-\\nods typically do not perform as well as full fine-tuning, leading to a trade-off\\nbetween efficiency and model performance.'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 1}, page_content='Inspired by prior works that demonstrate over-parametrized models often\\nreside in a low intrinsic dimensional space, we hypothesize that weight changes\\nduring model adaptation also have a low “intrinsic rank.” This insight leads to'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 1}, page_content='our Low-Rank Adaptation (LoRA) approach. LoRA optimizes low-rank decom-\\nposition matrices for the dense layers’ weight changes during adaptation, while\\nkeeping the pre-trained weights frozen. As illustrated in Figure 1, even with'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 1}, page_content='large models like GPT-3 (with up to 12,288 dimensions in full rank), a low-rank\\nmatrix (rank 1 or 2) is sufficient, making LoRA highly efficient in terms of both\\nstorage and computation.\\nLoRA has several notable advantages:'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 1}, page_content='•The pre-trained model can be shared, and small LoRA modules can be\\ncreated for various tasks. By freezing the main model and only switching\\nthe matrices AandB(shown in Figure 1), storage and task-switching\\noverhead are significantly reduced.'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 1}, page_content='•LoRA improves training efficiency and reduces hardware requirements,\\nlowering the entry barrier by up to threefold when using adaptive opti-\\nmizers. This is because LoRA only requires updating the smaller low-rank'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 1}, page_content='matrices, avoiding the need to calculate gradients for most parameters.\\n•The simple linear design allows merging of the trainable matrices with\\nthe frozen pre-trained weights during deployment, ensuring no additional'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 1}, page_content='inference latency compared to fully fine-tuned models.\\n•LoRA is compatible with many existing methods and can be combined\\nwith approaches like prefix-tuning.\\nIn this work, we follow standard conventions for Transformer architecture'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 1}, page_content='and refer to dimensions such as dmodel, and projection matrices like Wq,Wk,Wv,\\nandWofor the self-attention module. WorW0represents a pre-trained weight\\nmatrix, while ∆ Wrefers to its update during adaptation. The rank rdenotes'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 1}, page_content='the rank of a LoRA module. Throughout, we use Adam for optimization and\\nmaintain the Transformer MLP feedforward dimension as dffn= 4×dmodel.\\n1.1 Key Advantages of LoRA\\n•Efficient Task Switching : A pre-trained model can support multiple'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 1}, page_content='tasks by swapping the small LoRA matrices, reducing storage needs.\\n•Reduced Hardware Requirements : LoRA lowers the GPU memory\\nneeded for training by freezing most parameters and only training the\\nlow-rank matrices.'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 1}, page_content='low-rank matrices.\\n•No Additional Latency : LoRA incurs no extra inference delay because\\nthe matrices can be merged with the pre-trained weights when deployed.\\n2'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 2}, page_content='•Combining with Other Methods : LoRA can be used with other ap-\\nproaches, like prefix-tuning, to further optimize model performance.\\n2 Problem Statement\\nAlthough our approach is independent of the specific training objective, we focus'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 2}, page_content='on language modeling as the central application. Below, we outline the key\\naspects of the language modeling problem, particularly the goal of maximizing\\nconditional probabilities based on task-specific prompts.'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 2}, page_content='Assume we have an autoregressive language model PΦ(y|x) that is pre-\\ntrained and parameterized by Φ. For example, PΦ(y|x) could be a general\\nmulti-task model such as GPT, built on top of the Transformer architecture.'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 2}, page_content='The model can then be adapted to different downstream tasks such as text\\nsummarization, machine reading comprehension (MRC), and natural language\\nto SQL (NL2SQL). Each downstream task is represented as a training set of\\ncontext-output pairs:'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 2}, page_content='Z={(xi, yi)}i=1,...,N,\\nwhere both xiandyiare sequences of tokens. For instance, in NL2SQL, xi\\nmight represent a natural language question and yiwould be the corresponding\\nSQL query; in summarization, xirepresents the article and yiwould be its'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 2}, page_content='summary.\\nIn traditional fine-tuning, the model is initialized using the pre-trained weights\\nΦ0, which are then updated to Φ 0+ ∆Φ by optimizing the model’s parameters\\nto maximize the conditional probabilities for each token:\\nmax\\nΦX\\n(x,y)∈Z|y|X'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 2}, page_content='max\\nΦX\\n(x,y)∈Z|y|X\\nt=1log (PΦ(yt|x, y<t)) (1)\\nA significant limitation of full fine-tuning is that for every downstream task, a\\ndifferent set of parameters ∆Φ must be learned, and the size of ∆Φ is equal to the'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 2}, page_content='size of Φ 0. For large models, such as GPT-3 with 175 billion parameters, storing\\nand deploying multiple instances of fine-tuned models becomes impractical or\\nextremely challenging.'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 2}, page_content='To address this issue, we propose a more efficient approach where the task-\\nspecific parameter updates ∆Φ = ∆Φ(Θ) are encoded using a much smaller set\\nof parameters Θ, where |Θ| ≪ |Φ0|. As a result, optimizing the model for each'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 2}, page_content='task reduces to optimizing Θ as follows:\\nmax\\nΘX\\n(x,y)∈Z|y|X\\nt=1log\\x00\\npΦ0+∆Φ(Θ) (yt|x, y<t)\\x01\\n(2)\\nIn the following sections, we explore a low-rank approach for representing\\n∆Φ, making the adaptation process more efficient in both computational and'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 2}, page_content='memory terms. For large models like GPT-3 175B, this method allows the\\ntrainable parameters |Θ|to be reduced to as little as 0.01% of |Φ0|.\\n3'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 3}, page_content='3 Limitations on Current Solutions\\nThe challenge we aim to address is not new. Since the rise of transfer learning,\\na great deal of work has focused on making model adaptation more efficient in'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 3}, page_content='terms of both parameters and computation. For an overview, see Section 6 for\\nsome well-known works. Focusing on language modeling, two prominent strate-\\ngies for efficient adaptation stand out: adding adapter layers, or optimizing the'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 3}, page_content='input layer activations. However, both approaches come with limitations, espe-\\ncially when applied in large-scale, latency-sensitive production environments.\\n3.1 Adapter Layers and Inference Latency'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 3}, page_content='There are many variations of adapters. We focus on the original adapter design\\nfrom [ ?], which introduces two adapter layers per Transformer block, and a\\nmore recent approach by [ ?], which only uses one adapter per block but with an'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 3}, page_content='additional LayerNorm [ ?]. Although overall latency can be reduced by pruning\\nlayers or leveraging multi-task settings [ ?], [?], there is no way to completely\\neliminate the additional computation introduced by adapter layers. This might'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 3}, page_content='seem minor since adapters generally have few parameters (typically less than\\n1% of the original model) due to their small bottleneck dimension, which limits\\nthe number of floating-point operations (FLOPs). However, large-scale neural'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 3}, page_content='networks rely heavily on parallel processing to maintain low latency, and adapter\\nlayers are processed sequentially. This becomes more evident in scenarios with\\nlow batch sizes, such as real-time inference, where models like GPT-2 [ ?] running'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 3}, page_content='on a single GPU experience noticeable increases in latency, even with small\\nbottleneck dimensions (Table 1).\\nThe issue is further compounded when models need to be sharded across'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 3}, page_content='multiple devices, since the increased model depth requires more synchronous\\nGPU operations like AllReduce andBroadcast , unless adapter parameters are\\nredundantly replicated.\\n3.2 Challenges with Directly Optimizing the Prompt'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 3}, page_content='Another approach, such as prefix tuning [ ?], faces a different challenge. We\\nhave observed that prefix tuning is often difficult to optimize and that its per-\\nformance does not consistently improve as more trainable parameters are added,'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 3}, page_content='confirming earlier findings. Moreover, allocating part of the sequence length for\\nadaptation inevitably reduces the available sequence length for processing task-\\nrelated data, which seems to hinder prompt tuning’s performance compared to'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 3}, page_content='other methods. We will further explore this issue in Section 5.\\n4'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 4}, page_content='Batch Size Sequence Length |Θ| Latency (ms)\\nFine-Tune/LoRA\\n32 512 0.5M 1449.4 ±0.8\\n16 256 11M 338.0±0.6\\n1 128 11M 19.8±2.7\\nAdapterL\\n32 512 0.5M 1482.0 ±1.0 (+2.2%)\\n16 256 11M 354.8±0.5 (+5.0%)\\n1 128 11M 23.9±2.1 (+20.7%)\\nAdapterH'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 4}, page_content='AdapterH\\n32 512 0.5M 1492.2 ±1.0 (+3.0%)\\n16 256 11M 366.3±0.5 (+8.4%)\\n1 128 11M 25.8±2.2 (+30.3%)\\nTable 1: Inference latency of a forward pass in GPT-2 Medium measured over\\n100 trials using an NVIDIA Quadro RTX8000. ” |Θ|” refers to the number of'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 4}, page_content='trainable parameters in the adapter layers. AdapterLand AdapterHare two\\ntypes of adapter tuning. The impact on latency becomes significant, particularly\\nin online scenarios with shorter sequences and smaller batch sizes.\\n4 Our Method'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 4}, page_content='4 Our Method\\nIn this section, we explain the structure of LoRA and its practical benefits.\\nThe principles outlined here apply generally to dense layers in neural networks,'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 4}, page_content='although we focus on specific weights in Transformer language models, as these\\nmodels serve as the central example in our experiments.\\n4.1 Low-Rank Parameterized Update Matrices'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 4}, page_content='Neural networks contain numerous dense layers that perform matrix multipli-\\ncation, and the weight matrices in these layers typically have a full rank. When\\nadapting to a particular task, it shows that pre-trained language models pos-'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 4}, page_content='sess a low ”intrinsic dimension” and can still perform effectively after a random\\nprojection to a smaller subspace. Drawing inspiration from this, we hypothesize\\nthat updates to the weights during adaptation also have a low ”intrinsic rank.”'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 4}, page_content='For a pre-trained weight matrix W0∈Rd×k, we limit its update by express-\\ning it as a low-rank decomposition, W0+ ∆W=W0+BA, where B∈Rd×r,\\nA∈Rr×k, and the rank r≪min(d, k). During training, W0is fixed, and A'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 4}, page_content='andBare the trainable parameters. Both W0and ∆ W=BAare multiplied\\nwith the input, and their respective outputs are summed element-wise. Thus,\\nforh=W0x, our updated forward pass becomes:\\nh=W0x+ ∆Wx=W0x+BAx'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 4}, page_content='h=W0x+ ∆Wx=W0x+BAx\\nWe illustrate this reparametrization in Figure 1. We initialize Awith random\\nGaussian values and set Bto zero, meaning ∆ W=BAis zero at the start\\n5'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 5}, page_content='of training. We then scale ∆ Wxbyα\\nr, where αis a constant dependent on\\nr. When using Adam for optimization, adjusting αhas an effect similar to\\ntuning the learning rate. Therefore, we use the same αfor our first experiments'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 5}, page_content='and avoid tuning it. This scaling method also minimizes the need to adjust\\nhyperparameters when varying r.\\n4.1.1 A Generalization of Full Fine-Tuning\\nA more general fine-tuning technique involves training only a subset of pre-'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 5}, page_content='trained parameters. LoRA extends this approach by eliminating the need for\\nfull-rank gradient updates to weight matrices. Instead, LoRA uses low-rank\\nmatrices for adaptation. If LoRA is applied to all weight matrices, and all'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 5}, page_content='biases are trained, the expressiveness of full fine-tuning is recovered by setting\\nthe LoRA rank requal to the rank of the pre-trained weight matrices. As the\\nnumber of trainable parameters increases, LoRA approaches the full fine-tuning'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 5}, page_content='performance, while adapter-based techniques converge to simpler models that\\ncannot process long input sequences.\\n4.1.2 No Additional Inference Latency\\nWhen deploying LoRA, we can explicitly compute W=W0+BAand use it'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 5}, page_content='during inference. This means that when switching between tasks, we can quickly\\nsubtract BAand add a different low-rank matrix B′A′without consuming ex-\\ntra memory. This ensures that no additional inference latency is introduced'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 5}, page_content='compared to fully fine-tuned models.\\n4.2 Applying LoRA to Transformer Models\\nIn principle, LoRA can be applied to any subset of weight matrices in a neural\\nnetwork to minimize the number of trainable parameters. In a Transformer'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 5}, page_content='architecture, the self-attention module contains four projection matrices Wq,\\nWk,Wv, and Wo, and the MLP module contains two more matrices. We treat\\nthe weight matrices in the self-attention module as a single dmodel×dmodel'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 5}, page_content='matrix, despite them being split into different attention heads. To simplify\\nthe process and improve parameter efficiency, we restrict our method to only\\nadapting the attention weights for downstream tasks, leaving the MLP module'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 5}, page_content='frozen. The effect of adapting various attention weight matrices in a Transformer\\nis further explored in Section 7.1. We leave the investigation of adapting MLP\\nlayers, LayerNorm layers, and biases to future work.'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 5}, page_content='4.2.1 Practical Benefits and Limitations\\nOne of the major advantages of LoRA is its reduction in memory and storage\\ncosts. For large Transformer models using Adam, LoRA can cut VRAM usage'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 5}, page_content='by up to two-thirds if r≪dmodel, since it eliminates the need to store optimizer\\nstates for frozen parameters. For example, with GPT-3 175B, VRAM usage\\nduring training drops from 1.2 TB to 350 GB. With r= 4, and only the query\\n6'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 6}, page_content='and value matrices being adapted, the checkpoint size decreases by approxi-\\nmately 10,000 ×(from 350 GB to 35 MB). This makes it possible to train using\\nsignificantly fewer GPUs and avoid I/O bottlenecks. LoRA also enables easier'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 6}, page_content='task-switching during deployment by simply swapping out the LoRA weights,\\nwhich requires far less memory than loading entirely new model parameters.\\nAdditionally, LoRA offers a 25% training speedup compared to full fine-tuning'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 6}, page_content='because there is no need to compute gradients for most parameters.\\nHowever, LoRA does have some limitations. It is not straightforward to\\ncombine multiple tasks with different low-rank matrices AandBin a single'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 6}, page_content='forward pass if BAis absorbed into Wto remove additional inference latency.\\nWhile it is possible to dynamically select LoRA modules during inference, this\\nsolution is not suitable for scenarios where low-latency responses are crucial.'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 6}, page_content='5 Empirical Experiments\\nWe assess LoRA’s performance in downstream tasks across several models in-\\ncluding RoBERTa, DeBERTa, and GPT-2, before scaling up to GPT-3 175B.\\nOur experiments cover various tasks, ranging from natural language understand-'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 6}, page_content='ing (NLU) to natural language generation (NLG). For RoBERTa and DeBERTa,\\nwe evaluate on the GLUE benchmark. All experiments were performed using\\nNVIDIA Tesla V100 GPUs.\\n5.1 Baselines'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 6}, page_content='5.1 Baselines\\nFor comparison with a wide range of baselines, we replicate experimental setups\\nfrom previous studies and, where possible, reuse reported results. This might\\nresult in some baselines being present in only a subset of experiments.'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 6}, page_content='Fine-Tuning (FT) is a common method for adapting models. During fine-\\ntuning, the model’s pre-trained weights and biases are updated using gradient\\ndescent. A variant of this is fine-tuning only select layers, while freezing the'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 6}, page_content='rest. One such baseline from prior work on GPT-2 updates only the last two\\nlayers (denoted as FTTop2).\\nBitFit is another baseline in which only the bias parameters are updated,'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 6}, page_content='while all other parameters remain frozen. This method has gained attention,\\nincluding in recent studies [ ?].\\nPrefix-embedding tuning (PreEmbed) involves adding special tokens to'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 6}, page_content='the input sequence, and training their embeddings. These tokens do not belong\\nto the model’s original vocabulary. Their placement—either prepended (prefix)\\nor appended (infix)—can significantly affect performance, as highlighted in [ ?].'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 6}, page_content='Prefix-layer tuning (PreLayer) extends prefix tuning by learning train-\\nable activations at each Transformer layer. This results in a larger number of\\ntrainable parameters, as activations from prior layers are progressively replaced.'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 6}, page_content='The total number of trainable parameters is given by |Θ|=L×dmodel×(lp+li),\\nwhere Lis the number of Transformer layers.\\n7'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 7}, page_content='Adapter tuning [?] introduces additional fully connected adapter layers\\nbetween existing layers in the Transformer. Several variants exist, such as\\nAdapterH and AdapterL [ ?], which differ in the placement of adapters within'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 7}, page_content='the network. The number of trainable parameters in these methods is |Θ|=\\nLAdpt×(2×dmodel×r+r+dmodel) + 2×LLN×dmodel.\\nLoRA , on the other hand, introduces trainable low-rank matrices to the ex-'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 7}, page_content='isting weight matrices. As detailed in Section 4.2, LoRA is applied to the query\\nand value matrices in most experiments. The number of trainable parameters\\nis determined by the rank rand the shape of the original weight matrices:'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 7}, page_content='|Θ|= 2×LLoRA×dmodel×r, where LLoRA represents the number of weight\\nmatrices to which LoRA is applied.\\nTable 2: GPT-2 Medium and Large results on E2E NLG Challenge. Higher'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 7}, page_content='scores are better for all metrics. Confidence intervals are provided for experi-\\nments we conducted. *Results from prior work.\\nModel & Method # Trainable Parameters BLEU NIST MET ROUGE-L CIDEr\\nGPT-2 M (FT)* 354.92M 68.2 8.62 46.2 71.0 2.47'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 7}, page_content='GPT-2 M (LoRA) 0.35M 70.4±0.1 8.85±0.2 46.8±0.2 71.8±0.1 2.53±0.2\\nGPT-2 L (FT)* 774.03M 68.5 8.78 46.0 69.9 2.45\\nGPT-2 L (LoRA) 0.77M 70.4±0.1 8.89±0.2 46.8±0.2 72.0±0.2 2.47±0.2\\n5.2 Scaling LoRA to GPT-3 175B'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 7}, page_content='To further test LoRA’s scalability, we apply it to GPT-3 175B. Given the large\\ncomputational cost of GPT-3, we only report standard deviations for each task\\nbased on multiple random seeds. See Appendix D.4 for hyperparameters used.'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 7}, page_content='As presented in Table ??, LoRA matches or outperforms full fine-tuning on\\nWikiSQL, MultiNLI, and SAMSum. Notably, we observe that certain methods\\ndo not consistently benefit from increasing the number of trainable parame-'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 7}, page_content='ters. As shown in Figure 1, LoRA remains efficient even at low ranks, avoiding\\nthe performance degradation seen with larger token embeddings in prefix-based\\nmethods.\\nFigure 1: GPT-3 175B validation accuracy vs. the number of trainable parame-'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 7}, page_content='ters for several adaptation methods on WikiSQL and MNLI. LoRA demonstrates\\nbetter scalability and performance.\\n8'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 8}, page_content='6 Related Works\\n6.1 Transformer Language Models\\nThe Transformer architecture, as introduced by Vaswani et al. (2017), has\\nproven to be a highly effective sequence-to-sequence model due to its heavy use'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 8}, page_content='of self-attention mechanisms. Radford et al. (2018) applied it to autoregressive\\nlanguage modeling, significantly boosting its utility in the field. Since then,\\nTransformer-based models have become a staple in natural language processing'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 8}, page_content='(NLP), achieving state-of-the-art results in a wide variety of tasks. Notably,\\nBERT (Devlin et al., 2018) and GPT-2 (Radford et al., 2019) have paved the\\nway for large-scale pre-trained language models that, when fine-tuned, deliver'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 8}, page_content='excellent performance on specific tasks. The next breakthrough came with GPT-\\n3 (Brown et al., 2020), which is currently the largest single Transformer language\\nmodel with 175 billion parameters.\\n6.2 Prompt Engineering and Fine-Tuning'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 8}, page_content='Despite GPT-3’s ability to adapt its behavior with minimal data (few-shot learn-\\ning), its performance is highly sensitive to how the input prompt is structured\\n(Brown et al., 2020). This has led to the rise of ”prompt engineering,” a process'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 8}, page_content='that involves crafting and fine-tuning the input prompts to maximize model per-\\nformance on specific tasks. Fine-tuning, on the other hand, refers to retraining\\na model pre-trained on general domains to adapt it to a particular task (Devlin'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 8}, page_content='et al., 2018; Radford et al., 2018). Some approaches only update a subset of the\\nmodel’s parameters (Collobert and Weston, 2008), but it is common practice to\\nfine-tune all parameters to achieve the best performance. However, performing'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 8}, page_content='full fine-tuning on a model as large as GPT-3, with its 175 billion parameters,\\nposes significant challenges due to the large memory requirements and the com-\\nputational resources needed, making it as resource-intensive as pre-training.'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 8}, page_content='6.3 Parameter-Efficient Adaptation\\nMany techniques have been developed to address the inefficiency of full fine-\\ntuning by adapting only certain layers or introducing adapter modules. Houlsby'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 8}, page_content='et al. (2019), Rebuffi et al. (2017), and Lin et al. (2020) proposed inserting\\nadapter layers between existing layers in the network. These adapters allow for\\nparameter-efficient adaptation by learning only a small number of task-specific'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 8}, page_content='parameters. Our method imposes a low-rank constraint on the weight updates,\\nensuring that learned weights can be merged with the main model weights dur-\\ning inference, thus introducing no additional latency, unlike the adapter layers.'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 8}, page_content='A related approach, COMPACTER (Mahabadi et al., 2021), uses Kronecker\\nproducts to parametrize the adapters, further improving parameter efficiency.\\nAdditionally, prompt optimization techniques, such as those proposed by Li and'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 8}, page_content='Liang (2021), Lester et al. (2021), and Hambardzumyan et al. (2020), aim to\\noptimize the input tokens directly. However, these approaches typically reduce\\n9'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 9}, page_content='the available sequence length for task processing. Our work can be combined\\nwith such methods for further gains in efficiency.\\n6.4 Low-Rank Structures in Deep Learning\\nLow-rank structures are prevalent in many machine learning problems, and sev-'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 9}, page_content='eral studies have explored imposing these constraints on deep models. Li et\\nal. (2016), Cai et al. (2010), and Grasedyck et al. (2013) showed that many\\nlearning tasks have an intrinsic low-rank structure. For deep neural networks,'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 9}, page_content='particularly over-parameterized models, it has been shown that they often ex-\\nhibit low-rank properties after training (Oymak et al., 2019). Prior works, such\\nas those by Sainath et al. (2013), Zhang et al. (2014), and Denil et al. (2014),'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 9}, page_content='have explicitly imposed low-rank constraints during training to enhance model\\nefficiency. However, our approach differs in that we apply low-rank updates to\\nfrozen pre-trained models, making it highly effective for task-specific adapta-'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 9}, page_content='tion. Neural networks with low-rank structures have been shown to outperform\\nclassical methods such as finite-width neural tangent kernels (Allen-Zhu et al.,\\n2019; Li and Liang, 2018), and low-rank adaptations are particularly useful in'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 9}, page_content='adversarial training scenarios (Allen-Zhu and Li, 2020). This makes our pro-\\nposed low-rank adaptation well-grounded in both theory and practice.\\n7 Analyzing Low-Rank Adaptations'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 9}, page_content='In light of the demonstrated benefits of LoRA, we aim to further explore the\\nattributes of low-rank adaptation as applied to various downstream tasks. The\\nlow-rank structure does not only reduce the hardware requirements for conduct-'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 9}, page_content='ing parallel experiments, but it also provides better insight into how adapted\\nweights align with pre-trained weights. Our focus lies on GPT-3 175B, where\\nwe managed to significantly reduce the number of trainable parameters (up to'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 9}, page_content='10,000 ×) without sacrificing task performance.\\nIn this section, we address some key questions:\\n•1)With a constrained parameter budget, which weight matrices should\\nbe adapted to achieve the best downstream task performance?'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 9}, page_content='•2)Is the adapted matrix ∆ Wtruly rank-deficient, and if so, what rank is\\noptimal for practical use?\\n•3)How is ∆ Wrelated to the pre-trained weights W? Does ∆ Wexhibit\\nhigh correlation with W, and what is the comparative size of ∆ WtoW?'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 9}, page_content='The answers to these questions provide valuable insights for optimizing pre-\\ntrained models for downstream tasks.\\n7.1 Selecting Optimal Weight Matrices for LoRA\\nTo optimize performance under a limited parameter budget, we explore adapting'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 9}, page_content='different weight matrices within the self-attention module of the Transformer.\\n10'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 10}, page_content='We allocate 18M parameters (approximately 35MB stored in FP16) for GPT-3\\n175B, using a rank r= 8 for one attention weight type or r= 4 for two types.\\nThe results are displayed in Table 3.'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 10}, page_content='Table 3: Validation accuracy on WikiSQL and MultiNLI with LoRA applied to\\ndifferent attention weights in GPT-3, with a fixed number of trainable parame-\\nters.'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 10}, page_content='# of Trainable Parameters = 18M WqWkWvWoWq, Wv\\nRank r= 8 70.4 70.0 73.0 73.2 73.7\\nMultiNLI ( ±0.1%) 91.0 90.8 91.0 91.3 91.7\\n7.2 Determining the Ideal Rank for LoRA\\nTo analyze the impact of the rank ron task performance, we applied LoRA with'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 10}, page_content='varying ranks across different combinations of attention matrices. The results\\ncan be found in Table 4.\\nTable 4: Validation accuracy on WikiSQL and MultiNLI with different ranks r.\\nWeight Type r= 1 r= 2 r= 4 r= 8 r= 64'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 10}, page_content='WikiSQL ( ±0.5%)Wq68.8 69.6 70.5 70.4 70.0\\nWikiSQL ( Wq, Wv) 73.4 73.3 73.7 73.8 73.5\\nMultiNLI ( ±0.1%)Wq90.7 90.9 91.1 90.7 90.7\\nMultiNLI ( Wq, Wv) 91.3 91.4 91.3 91.6 91.4\\nThe results show that even at a small rank r= 1, LoRA performs well when'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 10}, page_content='both WqandWvare adapted. In contrast, adapting only Wqrequires a higher\\nrank for optimal performance.\\n8 Conclusion\\nLoRA offers a highly efficient solution to the problem of adapting large language'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 10}, page_content='models for downstream tasks. By freezing the majority of the model’s param-\\neters and training only small, low-rank matrices, LoRA achieves comparable\\nperformance to full fine-tuning while drastically reducing computational costs.'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 10}, page_content='Its ability to scale to massive models like GPT-3 without sacrificing performance\\nhighlights its potential for widespread use.\\nFuture work could explore combining LoRA with other parameter-efficient'),\n",
              " Document(metadata={'source': '/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf', 'page': 10}, page_content='methods or investigating more principled ways to select which weight matrices\\nto adapt. Additionally, further studies on the rank deficiency of pre-trained\\nweights could inspire new developments in efficient model adaptation.\\n11')]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(sample_chunks))\n",
        "print(sample_chunks[10].page_content)\n",
        "print(type(sample_chunks[10]))\n",
        "print(type(sample_chunks))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6EKbYanbAUBN",
        "outputId": "1b3071d0-a5e2-4ec7-a4d5-610f09ee3cf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "124\n",
            "model depth or reducing the usable sequence length. Furthermore, these meth-\n",
            "ods typically do not perform as well as full fine-tuning, leading to a trade-off\n",
            "between efficiency and model performance.\n",
            "<class 'langchain_core.documents.base.Document'>\n",
            "<class 'list'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embedding Model\n",
        "Embeddings are generated using IBM's Slate 125M English embeddings model.\n",
        "\n"
      ],
      "metadata": {
        "id": "N0w0aCMVeyHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Embedding model\n",
        "def watsonx_embedding():\n",
        "    embed_params = {\n",
        "        EmbedTextParamsMetaNames.TRUNCATE_INPUT_TOKENS: 3,\n",
        "        EmbedTextParamsMetaNames.RETURN_OPTIONS: {\"input_text\": True},\n",
        "    }\n",
        "    watsonx_embedding = WatsonxEmbeddings(\n",
        "        model_id=\"ibm/slate-125m-english-rtrvr\",\n",
        "        url=\"https://us-south.ml.cloud.ibm.com\",\n",
        "        project_id=\"skills-network\",\n",
        "        params=embed_params,\n",
        "    )\n",
        "    return watsonx_embedding"
      ],
      "metadata": {
        "id": "DlNmJnpJV5wz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vector Store\n",
        "Chroma vector store is used to store the generated vector embeddings"
      ],
      "metadata": {
        "id": "5DTPssQgA9li"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vector_database(chunks):\n",
        "    embedding_model = watsonx_embedding()\n",
        "    vectordb = Chroma.from_documents(chunks, embedding_model)\n",
        "    return vectordb"
      ],
      "metadata": {
        "id": "5xbysKXQAoiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db = vector_database(sample_chunks)"
      ],
      "metadata": {
        "id": "a0RkMnYAAoMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(db))\n",
        "print(type(db))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYDAVaRCFWEi",
        "outputId": "3e669b8f-f614-4391-e44c-f4278bbb8012"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "124\n",
            "<class 'langchain_community.vectorstores.chroma.Chroma'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Retriever\n",
        "A retriever is an interface designed to return documents based on an unstructured query. Unlike a vector store, which stores and retrieves documents, a retriever's primary function is to find and return relevant documents. While vector stores can serve as the backbone of a retriever, there are various other types of retrievers that can be used as well.\n",
        "\n",
        "Retrievers take a string `query` as input and output a list of `Documents`.\n"
      ],
      "metadata": {
        "id": "IuPiWJVNHNPk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Retriever\n",
        "def retriever(file):\n",
        "    splits = document_loader(file)\n",
        "    chunks = text_splitter(splits)\n",
        "    vectordb = vector_database(chunks)\n",
        "    retriever = vectordb.as_retriever(search_type=\"similarity\",search_kwargs={\"k\": 4})\n",
        "    return retriever"
      ],
      "metadata": {
        "id": "voxuvU_rFWBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Question-Answering Chain\n",
        "In this project, `RetrievalQA` from langchain, a chain that performs natural-language question-answering over a data source using retrieval-augmented generation (RAG), is used."
      ],
      "metadata": {
        "id": "oW-od915XM0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RetrievalQA Chain\n",
        "# LLM and retriever obj are taken by RetrievalQA\n",
        "def retriever_qa(doc, query):\n",
        "  llm= get_llm()\n",
        "  retriever_obj= retriever(doc)\n",
        "  qa= RetrievalQA.from_chain_type(llm=llm,\n",
        "                                  chain_type=\"stuff\",\n",
        "                                  retriever= retriever_obj,\n",
        "                                  return_source_documents=False)\n",
        "  result= qa.invoke(query)\n",
        "  return response['result']"
      ],
      "metadata": {
        "id": "gdVTTeNYFV-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set up the Gradio interface\n",
        "The web interface is user customizable and will have the following:\n",
        "*  A file upload functionality (provided by the `File` class in Gradio)\n",
        "*  An input textbox where the question can be asked (provided by the `Textbox` class in Gradio)\n",
        "* An output textbox where the question can be answered (provided by the `Textbox` class in Gradio)\n",
        "\n"
      ],
      "metadata": {
        "id": "hlgjN2GSj_ds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the Gradio interface\n",
        "rag_application= gr.Interface(\n",
        "    fn= retriever_qa,\n",
        "    allow_flagging=\"never\",\n",
        "    inputs=[\n",
        "    gr.File(label=\"Upload PDF File\", file_count=\"single\", file_types=['.pdf'], type=\"filepath\"),  # Drag and drop file upload\n",
        "        gr.Textbox(label=\"Input Query\", lines=2, placeholder=\"Type your question here...\")  # Input query box\n",
        "    ],\n",
        "    outputs= gr.Textbox(label=\"Summary of the Research Paper\"), # Output response box\n",
        "    title= \"Scholar Synopsis AI\",\n",
        "    description=\"Upload a research paper. The assistant can summarize and answer any questions you have to help you grasp key ideas and insights regarding the paper.\"\n",
        ")"
      ],
      "metadata": {
        "id": "3H377g4jFV6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qCmvh00tFVkG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}